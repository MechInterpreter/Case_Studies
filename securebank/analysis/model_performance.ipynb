{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72f3d810-51f3-4217-9228-96cce3a2c017",
   "metadata": {},
   "source": [
    "# Waldemar Chang - Assignment 3: Model Selection & Inference Pipeline\n",
    "## EN.705.603.82.FA24 Creating AI-Enabled Systems\n",
    "#### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bca70ed-8abe-4c50-ba27-dd1740dc588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All necessary imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from raw_data_handler import Raw_Data_Handler\n",
    "from dataset_design import Dataset_Designer\n",
    "from feature_extractor_2 import Feature_Extractor\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8596cd2c-2f63-41c6-a774-8856470b6ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transposing fraud_information to align with transaction_information\n",
      "No common column names found between fraud and trans data. Reverting to specific approach.\n",
      "Data successfully saved to output_data.parquet\n"
     ]
    }
   ],
   "source": [
    "# Data Pipeline\n",
    "\n",
    "# Raw Data Handler\n",
    "rdh = Raw_Data_Handler()\n",
    "e, x, t = rdh.extract('customer_release.csv', \n",
    "                      'transactions_release.parquet', \n",
    "                      'fraud_release.json')\n",
    "transformed = rdh.transform(e, x, t)\n",
    "rdh.load(output_filename=r'output_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8ec1aaf-aff8-4254-8647-99ccdaaef725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to partitioned.parquet\n"
     ]
    }
   ],
   "source": [
    "# Dataset Designer\n",
    "dsd = Dataset_Designer(test_size=0.2, target_column_name='fraudulence')\n",
    "extracted = dsd.extract('output_data.parquet')\n",
    "partitioned = dsd.sample(extracted)\n",
    "dsd.load('partitioned.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05a9b1a7-85f6-46ee-ad7d-dce188063ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial check for NaN values in training data:\n",
      "cc_num                         0\n",
      "index                          0\n",
      "first                    1185801\n",
      "last                     1185801\n",
      "sex                      1185801\n",
      "street                   1185801\n",
      "city                     1185801\n",
      "state                    1185801\n",
      "zip                      1185801\n",
      "lat                      1185801\n",
      "long                     1185801\n",
      "city_pop                 1185801\n",
      "job                      1185801\n",
      "dob                      1185801\n",
      "trans_num                      0\n",
      "trans_date_trans_time          0\n",
      "unix_time                 118754\n",
      "merchant                  118503\n",
      "category                  118960\n",
      "amt                            0\n",
      "merch_lat                      0\n",
      "merch_long                     0\n",
      "fraudulence                    0\n",
      "dtype: int64\n",
      "Initial check for NaN values in testing data:\n",
      "cc_num                        0\n",
      "index                         0\n",
      "first                    296396\n",
      "last                     296396\n",
      "sex                      296396\n",
      "street                   296396\n",
      "city                     296396\n",
      "state                    296396\n",
      "zip                      296396\n",
      "lat                      296396\n",
      "long                     296396\n",
      "city_pop                 296396\n",
      "job                      296396\n",
      "dob                      296396\n",
      "trans_num                     0\n",
      "trans_date_trans_time         0\n",
      "unix_time                 29742\n",
      "merchant                  29529\n",
      "category                  29490\n",
      "amt                           0\n",
      "merch_lat                     0\n",
      "merch_long                    0\n",
      "fraudulence                   0\n",
      "dtype: int64\n",
      "Check for NaN values after separating target in X_train:\n",
      "cc_num                         0\n",
      "index                          0\n",
      "first                    1185801\n",
      "last                     1185801\n",
      "sex                      1185801\n",
      "street                   1185801\n",
      "city                     1185801\n",
      "state                    1185801\n",
      "zip                      1185801\n",
      "lat                      1185801\n",
      "long                     1185801\n",
      "city_pop                 1185801\n",
      "job                      1185801\n",
      "dob                      1185801\n",
      "trans_num                      0\n",
      "trans_date_trans_time          0\n",
      "unix_time                 118754\n",
      "merchant                  118503\n",
      "category                  118960\n",
      "amt                            0\n",
      "merch_lat                      0\n",
      "merch_long                     0\n",
      "dtype: int64\n",
      "Check for NaN values in target Y_train:\n",
      "0\n",
      "Check for NaN values after separating target in X_test:\n",
      "cc_num                        0\n",
      "index                         0\n",
      "first                    296396\n",
      "last                     296396\n",
      "sex                      296396\n",
      "street                   296396\n",
      "city                     296396\n",
      "state                    296396\n",
      "zip                      296396\n",
      "lat                      296396\n",
      "long                     296396\n",
      "city_pop                 296396\n",
      "job                      296396\n",
      "dob                      296396\n",
      "trans_num                     0\n",
      "trans_date_trans_time         0\n",
      "unix_time                 29742\n",
      "merchant                  29529\n",
      "category                  29490\n",
      "amt                           0\n",
      "merch_lat                     0\n",
      "merch_long                    0\n",
      "dtype: int64\n",
      "Check for NaN values in target Y_test:\n",
      "0\n",
      "After dropping irrelevant features in X_train:\n",
      "sex                      1185801\n",
      "city                     1185801\n",
      "state                    1185801\n",
      "zip                      1185801\n",
      "lat                      1185801\n",
      "long                     1185801\n",
      "city_pop                 1185801\n",
      "job                      1185801\n",
      "dob                      1185801\n",
      "trans_date_trans_time          0\n",
      "unix_time                 118754\n",
      "merchant                  118503\n",
      "category                  118960\n",
      "amt                            0\n",
      "merch_lat                      0\n",
      "merch_long                     0\n",
      "dtype: int64\n",
      "After dropping irrelevant features in X_test:\n",
      "sex                      296396\n",
      "city                     296396\n",
      "state                    296396\n",
      "zip                      296396\n",
      "lat                      296396\n",
      "long                     296396\n",
      "city_pop                 296396\n",
      "job                      296396\n",
      "dob                      296396\n",
      "trans_date_trans_time         0\n",
      "unix_time                 29742\n",
      "merchant                  29529\n",
      "category                  29490\n",
      "amt                           0\n",
      "merch_lat                     0\n",
      "merch_long                    0\n",
      "dtype: int64\n",
      "After imputing numerical features in X_train:\n",
      "zip           0\n",
      "lat           0\n",
      "long          0\n",
      "city_pop      0\n",
      "unix_time     0\n",
      "amt           0\n",
      "merch_lat     0\n",
      "merch_long    0\n",
      "dtype: int64\n",
      "After imputing numerical features in X_test:\n",
      "zip           0\n",
      "lat           0\n",
      "long          0\n",
      "city_pop      0\n",
      "unix_time     0\n",
      "amt           0\n",
      "merch_lat     0\n",
      "merch_long    0\n",
      "dtype: int64\n",
      "After imputing categorical features in X_train:\n",
      "sex                      1185801\n",
      "city                     1185801\n",
      "state                    1185801\n",
      "job                      1185801\n",
      "dob                      1185801\n",
      "trans_date_trans_time          0\n",
      "merchant                  118503\n",
      "category                  118960\n",
      "dtype: int64\n",
      "After imputing categorical features in X_test:\n",
      "sex                      296396\n",
      "city                     296396\n",
      "state                    296396\n",
      "job                      296396\n",
      "dob                      296396\n",
      "trans_date_trans_time         0\n",
      "merchant                  29529\n",
      "category                  29490\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\walde\\securebank\\analysis\\feature_extractor_2.py:127: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  X_train['dob'] = pd.to_datetime(X_train['dob'], errors='coerce')\n",
      "C:\\Users\\walde\\securebank\\analysis\\feature_extractor_2.py:128: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  X_test['dob'] = pd.to_datetime(X_test['dob'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After feature engineering in X_train:\n",
      "sex                        1185801\n",
      "city                       1185801\n",
      "state                      1185801\n",
      "zip                              0\n",
      "lat                              0\n",
      "long                             0\n",
      "city_pop                         0\n",
      "job                        1185801\n",
      "unix_time                        0\n",
      "merchant                    118503\n",
      "category                    118960\n",
      "amt                              0\n",
      "merch_lat                        0\n",
      "merch_long                       0\n",
      "age                        1186154\n",
      "transaction_hour                 0\n",
      "transaction_day_of_week          0\n",
      "dtype: int64\n",
      "After feature engineering in X_test:\n",
      "sex                        296396\n",
      "city                       296396\n",
      "state                      296396\n",
      "zip                             0\n",
      "lat                             0\n",
      "long                            0\n",
      "city_pop                        0\n",
      "job                        296396\n",
      "unix_time                       0\n",
      "merchant                    29529\n",
      "category                    29490\n",
      "amt                             0\n",
      "merch_lat                       0\n",
      "merch_long                      0\n",
      "age                        296478\n",
      "transaction_hour                0\n",
      "transaction_day_of_week         0\n",
      "dtype: int64\n",
      "After normalizing numerical features in X_train:\n",
      "sex                                0\n",
      "city                         1185801\n",
      "zip                                0\n",
      "lat                                0\n",
      "long                               0\n",
      "                              ...   \n",
      "transaction_day_of_week_2          0\n",
      "transaction_day_of_week_3          0\n",
      "transaction_day_of_week_4          0\n",
      "transaction_day_of_week_5          0\n",
      "transaction_day_of_week_6          0\n",
      "Length: 99, dtype: int64\n",
      "After normalizing numerical features in X_test:\n",
      "sex                          0\n",
      "city                         0\n",
      "zip                          0\n",
      "lat                          0\n",
      "long                         0\n",
      "                            ..\n",
      "transaction_day_of_week_2    0\n",
      "transaction_day_of_week_3    0\n",
      "transaction_day_of_week_4    0\n",
      "transaction_day_of_week_5    0\n",
      "transaction_day_of_week_6    0\n",
      "Length: 99, dtype: int64\n",
      "Final check for NaN values in transformed train set (X_train):\n",
      "sex                                0\n",
      "city                         1185801\n",
      "zip                                0\n",
      "lat                                0\n",
      "long                               0\n",
      "                              ...   \n",
      "transaction_day_of_week_3          0\n",
      "transaction_day_of_week_4          0\n",
      "transaction_day_of_week_5          0\n",
      "transaction_day_of_week_6          0\n",
      "fraudulence                        0\n",
      "Length: 100, dtype: int64\n",
      "Final check for NaN values in transformed test set (X_test):\n",
      "sex                          0\n",
      "city                         0\n",
      "zip                          0\n",
      "lat                          0\n",
      "long                         0\n",
      "                            ..\n",
      "transaction_day_of_week_3    0\n",
      "transaction_day_of_week_4    0\n",
      "transaction_day_of_week_5    0\n",
      "transaction_day_of_week_6    0\n",
      "fraudulence                  0\n",
      "Length: 100, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Feature Extractor\n",
    "train_df = partitioned[partitioned['set_type'] == 'train'].drop(columns=['set_type'])\n",
    "test_df = partitioned[partitioned['set_type'] == 'test'].drop(columns=['set_type'])  \n",
    "fe = Feature_Extractor(target_column_name='fraudulence')\n",
    "tran = fe.transform(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90a7a2d3-eaf1-4a71-b361-df1a47b29d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = tran[0].drop(columns=['fraudulence'])\n",
    "y = tran[0]['fraudulence']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Transform training and testing data\n",
    "fe.fit_transformers(X_train)  # Fit transformers on training data\n",
    "X_train_transformed = fe.transform_data(X_train)\n",
    "X_test_transformed = fe.transform_data(X_test)\n",
    "\n",
    "# Define models to train\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(class_weight='balanced', random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42),\n",
    "    'Extra Trees': ExtraTreesClassifier(class_weight='balanced', random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20c67bde-4752-4a37-9b6b-78f3fc04b4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest...\n",
      "Random Forest saved to securebank/storage/models/artifacts\\Random Forest.joblib\n",
      "Random Forest - Accuracy: 0.9976\n",
      "Random Forest - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00    236225\n",
      "         1.0       0.90      0.52      0.66      1045\n",
      "\n",
      "    accuracy                           1.00    237270\n",
      "   macro avg       0.95      0.76      0.83    237270\n",
      "weighted avg       1.00      1.00      1.00    237270\n",
      "\n",
      "Random Forest - Confusion Matrix:\n",
      "[[236167     58]\n",
      " [   502    543]]\n",
      "\n",
      "Training Logistic Regression...\n",
      "Logistic Regression saved to securebank/storage/models/artifacts\\Logistic Regression.joblib\n",
      "Logistic Regression - Accuracy: 0.8936\n",
      "Logistic Regression - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.89      0.94    236225\n",
      "         1.0       0.03      0.73      0.06      1045\n",
      "\n",
      "    accuracy                           0.89    237270\n",
      "   macro avg       0.51      0.81      0.50    237270\n",
      "weighted avg       0.99      0.89      0.94    237270\n",
      "\n",
      "Logistic Regression - Confusion Matrix:\n",
      "[[211267  24958]\n",
      " [   284    761]]\n",
      "\n",
      "Training Extra Trees...\n",
      "Extra Trees saved to securebank/storage/models/artifacts\\Extra Trees.joblib\n",
      "Extra Trees - Accuracy: 0.9968\n",
      "Extra Trees - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00    236225\n",
      "         1.0       0.95      0.29      0.44      1045\n",
      "\n",
      "    accuracy                           1.00    237270\n",
      "   macro avg       0.97      0.64      0.72    237270\n",
      "weighted avg       1.00      1.00      1.00    237270\n",
      "\n",
      "Extra Trees - Confusion Matrix:\n",
      "[[236210     15]\n",
      " [   743    302]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model Training and Evaluation\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# Define the directory to save the models\n",
    "save_dir = 'securebank/storage/models/artifacts'\n",
    "os.makedirs(save_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_transformed, y_train)\n",
    "    \n",
    "    # Save the trained model\n",
    "    model_filename = os.path.join(save_dir, f'{model_name}.joblib')\n",
    "    joblib.dump(model, model_filename)\n",
    "    print(f\"{model_name} saved to {model_filename}\")\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test_transformed)\n",
    "    \n",
    "    # Generate classification report and confusion matrix\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    matrix = confusion_matrix(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Print the accuracy, classification report, and confusion matrix\n",
    "    print(f\"{model_name} - Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"{model_name} - Classification Report:\\n{report}\")\n",
    "    print(f\"{model_name} - Confusion Matrix:\\n{matrix}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de800270-8e22-4182-9d60-4c817e548ed0",
   "metadata": {},
   "source": [
    "- Out of the three models, Random Forest, Logistic Regression, and Extra Trees, I selected Random Forest to be the inference model due to its superior performance across the most common classification metrics.\n",
    "- Random Forest exhibited the best balance between precision, recall, and overall accuracy, making it a reliable choice.\n",
    "- It achieved an overall accuracy of 99.76%, surpassing both Logistic Regression (89.36%) and Extra Trees (99.68%).\n",
    "- While Extra Trees also performed well in terms of accuracy, Random Forest did better in handling the class imbalance in fraud detection.\n",
    "- Fraudulent transactions are rarer than legitimate ones, making it challenging for models to accurately predict fraud without producing many false positives.\n",
    "- Random Forest handled this challenge well, as demonstrated by its strong performance in detecting fraudulent cases.\n",
    "\n",
    "- Regarding recall, Random Forest identified 52% of fraudulent transactions, which is a significant improvement over Extra Trees, which only caught 29%.\n",
    "- While Logistic Regression had a higher recall of 73%, it suffered from low precision of 0.03, meaning it predicted a large number of false positives, flagging a large proportion of transactions as fraudulent.\n",
    "- Random Forest, on the other hand, balanced these metrics effectively, with a precision of 0.90 for fraud cases and an F1-score of 0.66.\n",
    "- This balance between precision and recall is crucial for SecureBankâ€™s fraud detection, as it reduces false positives while still identifying a significant portion of actual fraud cases.\n",
    "- Too many false positives can be inconvenient for customers and create extra work for investigators, while too few true positives can go on to cause significant harm.\n",
    "\n",
    "- In comparison, Extra Trees struggled with recall, identifying less than one-third of fraud cases, and while its precision was high at 0.95, its lower recall resulted in a significantly lower F1-score of 0.44.\n",
    "- This means that Extra Trees, while accurate in its predictions when it did identify fraud, missed a large number of fraudulent transactions, making it less effective in practice.\n",
    "- Logistic Regression, while able to catch more fraud cases, did so at the cost of falsely flagging many legitimate transactions, as indicated by its low F1-score of 0.06.\n",
    "  \n",
    "- Looking at the confusion matrix, Random Forest showed that it minimized both false positives and false negatives.\n",
    "- It identified 543 out of 1,045 fraud cases while only producing 58 false positives.\n",
    "- Although this could be far better, this level of performance is crucial in the context of fraud detection, where false positives can result in customer dissatisfaction and unnecessary interventions, while false negatives can lead to undetected fraudulent activity.\n",
    "- Logistic Regression, by contrast, produced an alarming 24,958 false positives, which would severely disrupt legitimate transactions.\n",
    "- Extra Trees had fewer false positives but struggled with 743 false negatives, meaning it missed a large number of fraud cases.\n",
    "\n",
    "- An additional point in favor of Random Forest is its training speed and computational efficiency.\n",
    "- While SVM was initially considered as an alternative model, its training time was prohibitively long due to the large dataset and the non-linear kernel it was using by default.\n",
    "- In contrast, Random Forest was significantly faster to train and evaluate, making it the most practical model for real-time fraud detection.\n",
    "- The faster runtime is especially important in environments where models need to be updated frequently or when large-scale data processing is required.\n",
    "\n",
    "- In conclusion, Random Forest emerged as the strongest model based on its ability to handle class imbalance, its high accuracy, and its balanced performance across precision, recall, and F1-score.\n",
    "- It offers the best trade-off between detecting fraud and minimizing false positives, making it the most reliable and robust model for deployment out of the three evaluated models.\n",
    "- Therefore, Random Forest is the optimal choice for the inference pipeline, ensuring both efficiency and effectiveness in detecting fraudulent transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5181af4-4260-4162-a3a3-742da127304c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
