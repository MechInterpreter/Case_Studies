{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "071041e0-aa4e-40dd-bfb3-83a574d1f886",
   "metadata": {},
   "source": [
    "# Waldemar Chang - Assignment 6: Integrating the Extraction and Retrieval Service\n",
    "## EN.705.603.82.FA24 Creating AI-Enabled Systems\n",
    "### Task 3: Analyze the pre-trained models (i.e., \"vggface2\" and \"casia-webface\")\n",
    "#### In a notebook called notebooks/embedding_analysis.ipynb, analyze the performance of the two models 'casia-webface' or 'vggface2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0c45b9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pipeline'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize the pipeline for both models (vggface2 and casia-webface)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m casia_pipeline \u001b[38;5;241m=\u001b[39m Pipeline(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcasia-webface\u001b[39m\u001b[38;5;124m'\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pipeline'"
     ]
    }
   ],
   "source": [
    "from pipeline import Pipeline\n",
    "\n",
    "# Initialize the pipeline for both models (vggface2 and casia-webface)\n",
    "casia_pipeline = Pipeline(pretrained='casia-webface', device='cpu')\n",
    "vggface2_pipeline = Pipeline(pretrained='vggface2', device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5fcbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Set up probe directory path\n",
    "probe_dir = \"simclr_resources/probe\"\n",
    "\n",
    "# Load probe images and labels from the directory\n",
    "test_images = []\n",
    "test_labels = []\n",
    "\n",
    "# Update the label extraction to use the parent directory (identity name)\n",
    "for root, _, files in os.walk(probe_dir):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(('.png', '.jpg', '.jpeg')) and not file.startswith('._'):\n",
    "            image_path = os.path.join(root, file)\n",
    "            test_images.append(Image.open(image_path))\n",
    "            identity_name = os.path.basename(os.path.dirname(image_path))  # Use the directory name as the label\n",
    "            test_labels.append(identity_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da01124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings for casia-webface model\n",
    "casia_embeddings = [casia_pipeline._encode(image) for image in test_images]\n",
    "\n",
    "# Extract embeddings for vggface2 model\n",
    "vgg_embeddings = [vggface2_pipeline._encode(image) for image in test_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1393dfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Initialize list to store inter-class similarities\n",
    "inter_similarities_casia = []\n",
    "inter_similarities_vgg = []\n",
    "\n",
    "# Compute inter-class similarities for Casia model\n",
    "for i in range(len(casia_embeddings)):\n",
    "    for j in range(i + 1, len(casia_embeddings)):\n",
    "        sim = cosine_similarity([casia_embeddings[i]], [casia_embeddings[j]])[0][0]\n",
    "        inter_similarities_casia.append(sim)\n",
    "\n",
    "# Compute inter-class similarities for VGG model\n",
    "for i in range(len(vgg_embeddings)):\n",
    "    for j in range(i + 1, len(vgg_embeddings)):\n",
    "        sim = cosine_similarity([vgg_embeddings[i]], [vgg_embeddings[j]])[0][0]\n",
    "        inter_similarities_vgg.append(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35db143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "# Define the noise levels to test\n",
    "noise_levels = [0.05, 0.1, 0.2]\n",
    "\n",
    "# Dictionaries to store similarities\n",
    "noise_similarities_casia = {}\n",
    "noise_similarities_vgg = {}\n",
    "\n",
    "for noise_level in noise_levels:\n",
    "    print(f\"\\nApplying Gaussian noise with standard deviation: {noise_level}\")\n",
    "    noise_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x + noise_level * torch.randn_like(x)),\n",
    "        transforms.ToPILImage()\n",
    "    ])\n",
    "    noisy_images = [noise_transform(image) for image in test_images]\n",
    "    \n",
    "    # Generate embeddings for noisy images (Casia)\n",
    "    noisy_casia_embeddings = [casia_pipeline._encode(image) for image in noisy_images]\n",
    "    \n",
    "    # Compare original and noisy embeddings (Casia)\n",
    "    casia_similarities = []\n",
    "    for i in range(len(casia_embeddings)):\n",
    "        sim = cosine_similarity([casia_embeddings[i]], [noisy_casia_embeddings[i]])[0][0]\n",
    "        casia_similarities.append(sim)\n",
    "    noise_similarities_casia[noise_level] = casia_similarities\n",
    "    \n",
    "    # Generate embeddings for noisy images (VGG)\n",
    "    noisy_vgg_embeddings = [vggface2_pipeline._encode(image) for image in noisy_images]\n",
    "    \n",
    "    # Compare original and noisy embeddings (VGG)\n",
    "    vgg_similarities = []\n",
    "    for i in range(len(vgg_embeddings)):\n",
    "        sim = cosine_similarity([vgg_embeddings[i]], [noisy_vgg_embeddings[i]])[0][0]\n",
    "        vgg_similarities.append(sim)\n",
    "    noise_similarities_vgg[noise_level] = vgg_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f31939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the kernel sizes to test\n",
    "blur_kernel_sizes = [3, 5, 7]\n",
    "\n",
    "# Dictionaries to store similarities\n",
    "blur_similarities_casia = {}\n",
    "blur_similarities_vgg = {}\n",
    "\n",
    "for kernel_size in blur_kernel_sizes:\n",
    "    print(f\"\\nApplying Gaussian blur with kernel size: {kernel_size}\")\n",
    "    blur_transform = transforms.GaussianBlur(kernel_size=kernel_size)\n",
    "    blurred_images = [blur_transform(image) for image in test_images]\n",
    "    \n",
    "    # Generate embeddings for blurred images (Casia)\n",
    "    blurred_casia_embeddings = [casia_pipeline._encode(image) for image in blurred_images]\n",
    "    \n",
    "    # Compare original and blurred embeddings (Casia)\n",
    "    casia_similarities = []\n",
    "    for i in range(len(casia_embeddings)):\n",
    "        sim = cosine_similarity([casia_embeddings[i]], [blurred_casia_embeddings[i]])[0][0]\n",
    "        casia_similarities.append(sim)\n",
    "    blur_similarities_casia[kernel_size] = casia_similarities\n",
    "    \n",
    "    # Generate embeddings for blurred images (VGG)\n",
    "    blurred_vgg_embeddings = [vggface2_pipeline._encode(image) for image in blurred_images]\n",
    "    \n",
    "    # Compare original and blurred embeddings (VGG)\n",
    "    vgg_similarities = []\n",
    "    for i in range(len(vgg_embeddings)):\n",
    "        sim = cosine_similarity([vgg_embeddings[i]], [blurred_vgg_embeddings[i]])[0][0]\n",
    "        vgg_similarities.append(sim)\n",
    "    blur_similarities_vgg[kernel_size] = vgg_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a07a178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the brightness factors to test\n",
    "brightness_factors = [0.5, 1.0, 1.5]\n",
    "\n",
    "# Dictionaries to store similarities\n",
    "brightness_similarities_casia = {}\n",
    "brightness_similarities_vgg = {}\n",
    "\n",
    "for factor in brightness_factors:\n",
    "    print(f\"\\nApplying brightness adjustment with factor: {factor}\")\n",
    "    brightness_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x * factor),\n",
    "        transforms.ToPILImage()\n",
    "    ])\n",
    "    bright_images = [brightness_transform(image) for image in test_images]\n",
    "    \n",
    "    # Generate embeddings for brightened images (Casia)\n",
    "    bright_casia_embeddings = [casia_pipeline._encode(image) for image in bright_images]\n",
    "    \n",
    "    # Compare original and brightened embeddings (Casia)\n",
    "    casia_similarities = []\n",
    "    for i in range(len(casia_embeddings)):\n",
    "        sim = cosine_similarity([casia_embeddings[i]], [bright_casia_embeddings[i]])[0][0]\n",
    "        casia_similarities.append(sim)\n",
    "    brightness_similarities_casia[factor] = casia_similarities\n",
    "    \n",
    "    # Generate embeddings for brightened images (VGG)\n",
    "    bright_vgg_embeddings = [vggface2_pipeline._encode(image) for image in bright_images]\n",
    "    \n",
    "    # Compare original and brightened embeddings (VGG)\n",
    "    vgg_similarities = []\n",
    "    for i in range(len(vgg_embeddings)):\n",
    "        sim = cosine_similarity([vgg_embeddings[i]], [bright_vgg_embeddings[i]])[0][0]\n",
    "        vgg_similarities.append(sim)\n",
    "    brightness_similarities_vgg[factor] = vgg_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a8dca2-293b-4b0a-861d-a7e376abc430",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4fc61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Casia boxplot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot([inter_similarities_casia], labels=['Inter-class'])\n",
    "plt.title('Cosine Similarity Distribution (Casia)')\n",
    "\n",
    "# VGG boxplot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot([inter_similarities_vgg], labels=['Inter-class'])\n",
    "plt.title('Cosine Similarity Distribution (VGG)')\n",
    "\n",
    "# Adjust layout and show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2430a66-242c-42a2-b1ec-c8c230a1f746",
   "metadata": {},
   "source": [
    "### General Model Performance\n",
    "\n",
    "The general prediction model performance for both Casia and VGGFace2 models is shown through the cosine similarity distributions of inter-class embeddings. A boxplot is used to visualize these distributions, which provides important insights into the performance of the models by showing the spread and central tendency of the similarity scores between different identities.\n",
    "\n",
    "A boxplot represents data through several key summary statistics: the minimum, first quartile (Q1), median (Q2), third quartile (Q3), and maximum. The box represents the interquartile range (IQR) between Q1 and Q3, with the line inside the box showing the median (Q2). The whiskers extend to the minimum and maximum values, excluding outliers, which are represented as individual points outside the whiskers. By analyzing the boxplots for both models, we can infer their ability to distinguish between different identities.\n",
    "\n",
    "For the Casia model, the median cosine similarity score is around 0.6. This indicates that on average, embeddings of different identities are somewhat similar. A higher median similarity implies that the different identities are not as well separated, suggesting less effective discrimination between individuals. Additionally, the spread of the similarity values for Casia is relatively narrow, indicating consistent but less distinctive embeddings across different identities. The outliers near 0 indicate a few cases where the model achieved better separation, but these instances are less common.\n",
    "\n",
    "In contrast, the VGG model has a median cosine similarity score close to 0, which implies that embeddings of different identities are much less similar. This lower median indicates better separation between different identities, which is desirable for face recognition. The spread of the similarity values for VGG is wider, meaning that the model generates a broader range of similarity scores for different identities, reflecting its ability to create more unique features for each person. The outliers in the VGG plot show a larger number of negative similarity values, which suggests that the model frequently produces distinct embeddings, thereby better distinguishing between different individuals.\n",
    "\n",
    "In summary, the Casia model exhibits a higher median inter-class similarity around 0.6, indicating less separation between different identities, and has a narrower spread, implying less distinctive embeddings. On the other hand, the VGG model shows a median inter-class similarity close to 0, with a wider spread and more negative similarity values, suggesting that it is more effective in distinguishing between different identities.\n",
    "\n",
    "These insights into the cosine similarity distributions help us understand that the VGG model is generally better at creating distinctive embeddings for different identities, which is crucial for accurate face recognition. The Casia model might require additional improvements or preprocessing to enhance its distinctiveness, while VGG demonstrates greater robustness in separating identities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70bd9fa-bcd8-469b-b70f-a11d76d32614",
   "metadata": {},
   "source": [
    "### Impact of Noise Transformations\n",
    "\n",
    "The robustness of both models to different types of noise transformations is evaluated by analyzing the change in cosine similarity scores under different noise levels. Below are the observations for Gaussian noise, Gaussian blur, and brightness adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad40f455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the impact of Gaussian noise on similarity scores\n",
    "\n",
    "# Prepare data for plotting\n",
    "noise_levels_list = list(noise_similarities_casia.keys())\n",
    "casia_means = [np.mean(noise_similarities_casia[nl]) for nl in noise_levels_list]\n",
    "vgg_means = [np.mean(noise_similarities_vgg[nl]) for nl in noise_levels_list]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(noise_levels_list, casia_means, label='Casia', marker='o')\n",
    "plt.plot(noise_levels_list, vgg_means, label='VGG', marker='o')\n",
    "plt.xlabel('Gaussian Noise Standard Deviation')\n",
    "plt.ylabel('Mean Similarity Score')\n",
    "plt.title('Model Robustness to Gaussian Noise')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47087b8b-6270-4ecc-a94d-d878dd93a145",
   "metadata": {},
   "source": [
    "#### 1. **Gaussian Noise**\n",
    "\n",
    "The similarity scores between the original images and those with Gaussian noise are plotted above. It can be observed that:\n",
    "\n",
    "- **VGG**: Initially, VGG has a higher similarity score compared to Casia. However, as the noise level increases, the performance of the VGG model deteriorates more sharply, indicating a higher sensitivity to Gaussian noise.\n",
    "- **Casia**: Although Casia starts with a slightly lower similarity score, it maintains a more gradual decline in performance, showing more robustness to increasing levels of Gaussian noise compared to VGG.\n",
    "\n",
    "**System Design Impact**: Given the sensitivity of the VGG model to Gaussian noise, a system designed with VGG might require noise reduction preprocessing steps to ensure consistent performance. Casia, being more resilient, might be better suited for environments where noise is a concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6727fd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Gaussian Blur\n",
    "kernel_sizes = list(blur_similarities_casia.keys())\n",
    "casia_means_blur = [np.mean(blur_similarities_casia[ks]) for ks in kernel_sizes]\n",
    "vgg_means_blur = [np.mean(blur_similarities_vgg[ks]) for ks in kernel_sizes]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(kernel_sizes, casia_means_blur, label='Casia', marker='o')\n",
    "plt.plot(kernel_sizes, vgg_means_blur, label='VGG', marker='o')\n",
    "plt.xlabel('Gaussian Blur Kernel Size')\n",
    "plt.ylabel('Mean Similarity Score')\n",
    "plt.title('Model Robustness to Gaussian Blur')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7146f9f0-8e19-4920-be3f-a9f347611045",
   "metadata": {},
   "source": [
    "#### 2. **Gaussian Blur**\n",
    "\n",
    "The impact of Gaussian blur is shown in Figure 3. Both models show a decline in similarity scores as the blur kernel size increases, indicating a reduction in performance under blurring.\n",
    "\n",
    "- **VGG**: The similarity score decreases faster for the VGG model as the blur kernel size increases, showing that VGG is more sensitive to image blurriness.\n",
    "- **Casia**: The decline is less steep, implying that Casia is relatively more robust to blur, although the performance still degrades with increasing blur severity.\n",
    "\n",
    "**System Design Impact**: In scenarios where images may be out of focus (e.g., surveillance cameras), the Casia model might be a more reliable choice due to its robustness to blur. Alternatively, using deblurring techniques can improve VGG performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa01c45-7ce5-4f77-8fa5-0fee0a2153af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Brightness Adjustments\n",
    "brightness_levels = list(brightness_similarities_casia.keys())\n",
    "casia_means_brightness = [np.mean(brightness_similarities_casia[bf]) for bf in brightness_levels]\n",
    "vgg_means_brightness = [np.mean(brightness_similarities_vgg[bf]) for bf in brightness_levels]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(brightness_levels, casia_means_brightness, label='Casia', marker='o')\n",
    "plt.plot(brightness_levels, vgg_means_brightness, label='VGG', marker='o')\n",
    "plt.xlabel('Brightness Factor')\n",
    "plt.ylabel('Mean Similarity Score')\n",
    "plt.title('Model Robustness to Brightness Adjustments')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bf1cde-1457-4d77-863e-1fd10772d34c",
   "metadata": {},
   "source": [
    "#### 3. **Brightness Adjustments**\n",
    "\n",
    "Figure 4 shows the models' performance under brightness adjustments. The similarity scores for both models peak when the brightness factor is close to the original (1.0).\n",
    "\n",
    "- **VGG**: Exhibits a dramatic drop in performance when the brightness is altered, indicating a high sensitivity to changes in brightness.\n",
    "- **Casia**: The Casia model also shows a decline, but the drop is more controlled, especially when the brightness is increased. This indicates that Casia is better at handling changes in illumination.\n",
    "\n",
    "**System Design Impact**: For environments with highly variable lighting, such as outdoor settings, the Casia model would perform more consistently. Alternatively, brightness normalization techniques could be employed to improve VGG's performance in such conditions.\n",
    "\n",
    "### Conclusions and Recommendations\n",
    "\n",
    "- **Model Selection**: Based on the analysis, the Casia model demonstrates better robustness across different types of noise, including Gaussian noise, blur, and brightness changes. This makes it a more suitable candidate for real-world applications where image quality may vary.\n",
    "- **Preprocessing**: If the VGG model is used, additional preprocessing steps such as noise reduction, deblurring, and brightness normalization should be considered to mitigate the impact of these noise factors.\n",
    "- **System Thresholds**: The similarity score thresholds for decision-making (e.g., identity verification) should be adjusted based on expected noise levels and transformations to minimize false positives or negatives.\n",
    "\n",
    "In summary, while both models have their strengths, the Casia model's robustness to different noise conditions makes it better suited for deployment in challenging environments where the image quality cannot be guaranteed. However, if VGG is preferred for its better inter-class separation, adequate preprocessing steps should be taken to ensure stable performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34ba401-73a7-4f9c-b0d8-070ea4b1887f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
