# raw_data_handler.py

### __init__()
The decision to use a constructor was made to support state transfer for the ETL (Extract-Transform-Load) process, which is commonly used in Data Engineering.

### extract()
The original intent was to transpose the JSON file, turning its columns into rows to align the structure of the fraud data with the transaction data for merging. This approach was based on the assumption that the fraud data would always have the same structure, requiring transposition before merging. However, I decided to generalize the JSON handling to make the pipeline more adaptable. The assignment instructions suggested building the module for general machine learning purposes and not just for the fraud detection use case, so I adhered to this constraint. The transposition was moved down the pipeline to the `transform()` function, where it would be conditionally triggered. The decision to flatten the JSON structure using `pandas.json_normalize()` was aimed at handling potential nested structures, allowing the function to work with various JSON formats.

### transform()
The original approach was tailored specifically to the fraud detection dataset, which involved hardcoding the column names and rules specific to that dataset. This approach was retained as a fallback mechanism, given that we are currently working with this dataset, but a dynamic layer was developed on top of it. This dynamic layer attempts to find matching columns between the datasets by checking for common column names and merging based on those. At its core, this generalizes the merging logic, allowing the pipeline to adapt to variations in column names across different datasets without needing to rewrite the code for each new case. Additionally, after merging the datasets, the function standardizes column names by converting them to lowercase and replacing spaces with underscores. This ensures consistency and avoids issues related to case sensitivity or spaces in column names. After that, the function performs essential data cleaning operations by removing duplicate rows and dropping rows and columns that are entirely empty. These steps ensure the dataset is clean, well-structured, and ready for further analysis or machine learning tasks.

### describe()
The `describe()` function was designed to calculate key quality metrics for the transformed dataset. Initially, I considered adding predefined quality checks tailored specifically to the fraud detection dataset, but I decided to generalize it to make it useful for any machine learning dataset. The function computes completeness, size, accuracy (via z-score-based outlier detection), timeliness, and representativeness. These metrics are critical for understanding the dataset's quality for machine learning tasks, regardless of the specific domain. The generalization of representativeness (e.g., Wasserstein distance, KL divergence) ensures that the function can assess the fairness of any dataset where distributions can be split into two groups. The use of `**kwargs` allows versioning and file path metadata to be incorporated dynamically without hardcoding these values. For timeliness and representativeness, I conditionally hardcoded specific column names (e.g., `trans_date_trans_time` for timeliness and `fraudulence` for distribution analysis) because I could not assume that other datasets would always include a datetime column or a binary column. This decision ensures the function works correctly for the current fraud detection dataset while allowing for future extensions. By hardcoding these columns, I guarantee that the function can deliver accurate metrics for particular datasets where these fields are present, without risking errors or inconsistencies in cases where such columns are absent.

### load()
The `load()` function is designed to save the transformed DataFrame (`self.merged_df`) in Parquet format. The function checks if the transform() function has been run before saving, ensuring that unprocessed data is not saved. The design also includes basic error handling to ensure the data is written properly.


# dataset_design.py

### __init__()
I decided to use a constructor and set `test_size` and `target_column_name` at initialization to ensure consistent handling of the dataset throughout its lifecycle. By requiring `test_size` and `target_column_name` as parameters, I made sure that these critical components are stored as attributes, allowing the class to keep track of the dataset configuration throughout the workflow. I also initialized `self.df` to `None`, as it is going to be used later after the data has been processed and split. My intention here was to allow the data to flow smoothly from extraction to final output, with consistency at each step.

### extract()
The `extract()` method is responsible for reading raw data from a Parquet file and returning it as a DataFrame. This method is straightforward and focuses on ingestion, ensuring that the data is ready for processing in subsequent steps of the pipeline. The `extract()` method is aligned with the flexibility required for general machine learning tasks, avoiding hardcoded logic specific to any particular use case, and ensuring reusability across different datasets.

### sample()
The purpose of `sample()` is to split the dataset into training and testing sets using the `train_test_split` method from `sklearn`. When I designed this, I wanted to make sure the split was straightforward but also provide a way to keep track of the split. So, I decided to create a new column called `set_type` that labels each record as either `train` or `test`. This allows me to work with a combined dataset later on, but still easily differentiate between training and test data. Once the split is done, I combine the data and store it in `self.df`, which becomes the central DataFrame I work with throughout the rest of the pipeline. I wanted this function to provide an organized output that makes it easy to reference training and testing data, without needing to juggle multiple DataFrames.

### describe()
I’ve already discussed the design of the `describe()` function in more detail when describing the `raw_data_handler.py` program. In this case, I’ve reused the same logic here because it’s flexible and general enough to provide meaningful insights for any machine learning dataset, not just fraud detection. The key metrics—completeness, size, accuracy, timeliness, and representativeness—are essential when assessing the quality of a dataset for model training and evaluation. These metrics remain consistent in their utility regardless of the dataset’s domain, so the function can be easily applied across different projects without much modification. By reusing this logic, I wanted to ensure that the function provides a consistent, reliable method for evaluating dataset quality across a range of different machine learning contexts.

### load()
Like the `load()` function in the `raw_data_handler.py` program, I added a simple check to ensure that the `sample()` function has been run, which avoids the risk of saving an unprocessed or empty dataset. If everything is in order, it saves the DataFrame to Parquet format.


# feature_extractor.py

### __init__()
In the `Feature_Extractor` class, I decided to keep things focused on the target column again by requiring `target_column_name` as an initialization parameter. This ensures that the class always knows which column to treat as the target variable for model training. By storing this as an attribute, it simplifies the process of extracting, transforming, and describing the data later on, without needing to repeatedly specify the target. I wanted the class to be flexible yet clear, so setting the target at initialization was key to maintaining a consistent approach across the various methods that handle feature extraction.

### extract()
The `extract()` method is responsible for reading in the provided datasets for both training and testing. The method accepts the file paths for the training and testing datasets as arguments and returns them as pandas DataFrames to be transformed into useful features by the `transform()` function. By returning the datasets as DataFrames, I enable them to be immediately passed to downstream transformations (`transform()` method). Since this method is straightforward and reusable, it can easily be adapted to read data from other file formats in the future if necessary.

### transform()
The `transform()` method handles the conversion of raw datasets into features that are useful for training. It accepts the training and testing DataFrames as input, separates the features from the target column, and applies transformations like scaling to the numerical features. By leveraging StandardScaler, I wanted to ensure that the numerical data is standardized for model training. The focus here is to prepare the data for numerical analysis, so I intentionally excluded categorical features, which aren't required for tasks like generating correlation matrices. The method returns a list containing the transformed training and testing DataFrames, preserving the column structure for readability and further processing.

### describe()
The `describe()` method computes significant quality metrics for the transformed datasets. It accepts multiple DataFrames as input and returns a detailed summary that includes a new key metric: the Pearson correlation matrix. The generation of this correlation matrix helps assess the relationships between the numerical features extracted in the previous `transform()` method. The correlation matrix provides a powerful way to compare the relevancy of the features by quantifying how closely they are related to one another, which is essential for understanding which features might be more predictive of the target variable. This makes it easier to spot multicollinearity issues or identify which features to prioritize in downstream model development. It also includes the reused quality metrics from `raw_data_handler.py` and `dataset_design.py` such as correlation, completeness, accuracy, timeliness, and representativeness. The method returns a list of structured dictionaries that include version information, storage details, and a comprehensive description of the dataset's quality metrics. The list compiles the structured dictionaries corresponding to the training and test DataFrames.